{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asritha0606/GenerativeAI/blob/main/GenAI_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio: For deploying your apps"
      ],
      "metadata": {
        "id": "v2AflNldgVja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio groq litellm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WIM-ynKOHH6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7eb321e-bb3a-42bb-8993-351c3d3f36dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api')\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
        "\n",
        "\n",
        "hf_api_key = userdata.get('HF_TOKEN')\n",
        "os.environ[\"HUGGINGFACEHUB_API_KEY\"] = hf_api_key\n",
        "\n",
        "openai_key = userdata.get(\"open_ai\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key"
      ],
      "metadata": {
        "id": "9J5zN7oUX8VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input- My text\n",
        "\n",
        "Prompt- custom output\n",
        "\n",
        "Function- to test the prompt\n",
        "\n",
        "Check output\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Import & Create Gradio interface\n",
        "\n",
        "Function\n",
        "\n",
        "Input\n",
        "\n",
        "Output\n",
        "\n",
        "Add decoratives\n",
        "\n",
        "Launch the interface\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7wiFHoOAohTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_text1=\"\"\"\n",
        "The black box nature of machine learning poses a significant challenge, as it obscures the decision-making process of complex models,\n",
        "making them difficult to interpret and trust. While these models can achieve remarkable accuracy, but their lack of transparency makes\n",
        "them opaque.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9vQrIAScYmTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "from litellm import completion\n",
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "def human_or_ai(input_text):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Following is a piece of text {input_text}. Your task is to determine whether it is written by a human or AI.\n",
        "    Instructions:\n",
        "    1. Carefully read the given text.\n",
        "    2. Analyse the writing style, grammar and english vocabulary.\n",
        "    3. Based on your analysis, only return response as \"Human\" or \"AI\".\n",
        "    4. If you do not know the answer, just say \"I am not sure, sorry!\"\n",
        "    5. Remember to return your answer as \"Human\" or \"AI\" based on your analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    model= \"groq/llama-3.1-70b-versatile\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    client = Groq(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        "    )\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        "    )\n",
        "    result= chat_completion.choices[0].message.content\n",
        "    return result\n",
        "\n",
        "output = human_or_ai(my_text1)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_g-Sr8QYptk",
        "outputId": "0a36a60e-af50-474d-e910-4b71b4f9a3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After carefully reading the text and analyzing its writing style, grammar, and vocabulary, I would say that this text is likely written by an \"AI\".\n",
            "\n",
            "Here's why:\n",
            "\n",
            "* The text is straightforward and lacks nuance in its language, which is a characteristic often associated with AI-generated text.\n",
            "* The sentence structure and vocabulary are simple and formal, which is consistent with the typical output of AI writing tools.\n",
            "* The text doesn't have a distinct personality, tone, or voice that is often present in human-written text.\n",
            "* The language is not rich or descriptive, and the ideas are presented in a somewhat mechanical way, which is consistent with AI-generated content.\n",
            "\n",
            "Of course, I could be wrong! But based on my analysis, I think it's a safe bet to say that this text was likely written by an AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# Create a Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=human_or_ai,  # The function to display output from\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter some text here...\"),  # Input type\n",
        "    outputs=gr.Textbox(),  # Output type\n",
        "    title=\"Human or AI:Who said it actually?\",\n",
        "    # description=\"Who said it actually?\",\n",
        "    # examples=examples  # Include examples in the interface\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "LxF1FpCkYw7a",
        "outputId": "1f11a600-0bec-457d-9001-c35b84494698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6d2511a02ceec30582.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6d2511a02ceec30582.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding examples"
      ],
      "metadata": {
        "id": "3paj6rTZZwaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pre-saved examples\n",
        "examples = [\n",
        "    [\"The black box nature of machine learning poses a significant challenge, as it obscures the decision-making process of complex models, making them difficult to interpret and trust. While these models can achieve remarkable accuracy, their lack of transparency makes them opaque.\"],\n",
        "    [\"Climate change is the most pressing issue of our time. We need to take immediate action to reduce carbon emissions and transition to renewable energy. Governments and corporations must lead the charge, but individual actions also matter. Every small step counts in the fight against climate change.\"],\n",
        "    [\"Join a community of Like-minded people empowering each other to thrive at every stage of life!​ Sign up for our newsletter and receive exclusive health tips, inspiring stories and the latest​ research to elevate your mental well-being and celebrate your age with confidence and joy.\"],\n",
        "    [\"'Clueless about Sustainability? Get your Eco-Guide inside'\"],\n",
        "    [\"Pizza is universally loved because it's the perfect balance of flavor, comfort, and simplicity. A slice can turn any bad day around.\"],\n",
        "    [\"Honestly, I think pizza is the best comfort food. Nothing beats a hot slice after a long day\"],\n",
        "    [\"Existence is a canvas upon which we paint our thoughts and feelings, transforming the abstract into something tangible, something meaningful.\"],\n",
        "    [\"It is important to encourage conversation around mental health so that people feel safe to discuss and get solutions for their issues.\"],\n",
        "    [\"Adopting a healthy diet is more than just a trend; it's a lifestyle choice that can lead to long-term benefits. Focusing on whole foods and reducing processed items is key, not only for our well-being but also for the health of the planet.​\"]\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "dnOrnOUDZX9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# Create a Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=human_or_ai,  # The function to display output from\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter some text here...\"),  # Input type\n",
        "    outputs=gr.Textbox(),  # Output type\n",
        "    title=\"Human or AI?\",\n",
        "    description=\"Who said it actually?\",\n",
        "    examples=examples  # Include examples in the interface\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "R_L4o6sgZe1t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "e41a38ff-3021-4bf2-c57d-8fa75dcd1d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://caf614f6aa3d0a4334.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://caf614f6aa3d0a4334.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload a file and performing a function"
      ],
      "metadata": {
        "id": "rQqq5fBtlTOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/Generative-AI.pdf\"\n",
        "def convert_to_uppercase(path):\n",
        "    # Read the uploaded file\n",
        "    with open(path, 'r') as file:\n",
        "        content = file.read()\n",
        "    upper_case_content = content.upper()\n",
        "    return upper_case_content\n",
        "\n",
        "text=convert_to_uppercase(path)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "6Uh8FpHvlzW8",
        "outputId": "8e8536a4-4bb2-4fb9-827d-d77c5e36742e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xc7 in position 10: invalid continuation byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a13f366f0966>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupper_case_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_uppercase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a13f366f0966>\u001b[0m in \u001b[0;36mconvert_to_uppercase\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Read the uploaded file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mupper_case_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupper_case_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc7 in position 10: invalid continuation byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q  PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sgV1zSvga-t",
        "outputId": "461876e5-c5b2-41de-d390-43259c3786ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "path = \"/content/Generative-AI.pdf\"\n",
        "\n",
        "def convert_to_uppercase(path):\n",
        "    \"\"\"Reads text from a PDF file and converts it to uppercase.\"\"\"\n",
        "    content = \"\"\n",
        "    try:\n",
        "        with open(path, 'rb') as file:  # Open in binary read mode\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                page = reader.pages[page_num]\n",
        "                content += page.extract_text() or \"\" # Extract text and handle potential None\n",
        "\n",
        "        upper_case_content = content.upper()\n",
        "        return upper_case_content\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF file: {e}\"\n",
        "\n",
        "text = convert_to_uppercase(path)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW8kDtG-gYvE",
        "outputId": "300bde38-969f-465b-8aae-17bf656bbd71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATIVE AI: \n",
            "AN OVERVIEW \n",
            "UNDERSTANDING RECURRENT NEURAL NETWORKS (RNNS)\n",
            "RNNS ARE A TYPE OF NEURAL NETWORK. \n",
            "THEY ARE DESIGNED TO PROCESS \n",
            "SEQUENTIAL DATA.\n",
            "THESE ARCHITECTURES WERE WIDELY \n",
            "USED FOR NLP TASKS, SPEECH \n",
            "PROCESSING, AND TIME SERIES. \n",
            "CHALLENGE-?\n",
            "THE RISE OF TRANSFORMERS: SELF-ATTENTION\n",
            "IN 2017, RESEARCHERS AT GOOGLE \n",
            "PUBLISHED A PAPER THAT PROPOSED A \n",
            "NOVEL NEURAL NETWORK ARCHITECTURE \n",
            "FOR SEQUENCE MODELING KNOWN AS \n",
            "TRANSFORMER.\n",
            "OUTPERFORMED RECURRENT NEURAL \n",
            "NETWORKS (RNNS) ON MACHINE \n",
            "TRANSLATION TASKS, BOTH IN TERMS OF \n",
            "TRANSLATION QUALITY AND TRAINING COST.\n",
            "A TIMELINE OF LARGE LANGUAGE MODELS\n",
            "2022: CHATGPT\n",
            "GENERATIVE PRE-TRAINED TRANSFORMER 2.\n",
            "2024: META'S LLAMA 3, CLAUDE 3, AND Q2, AND MISTRAL'S MIXTRAL 8X7B\n",
            "LARGER AND MORE POWERFUL MODEL.\n",
            "2025: DEEPSEEK-R1\n",
            "MULTIMODALITY: T EXT, IMAGE, VIDEO\n",
            "DIVING INTO CHATGPT\n",
            "GENERATIVE\n",
            "NEXT WORD PREDICTION\n",
            "PRE-TRAINED\n",
            "LLM IS PRE-TRAINED ON MASSIVE \n",
            "AMOUNT OF TEXT\n",
            "TRANSFORMER\n",
            "ENCODER-DECODER ARCHITECTURE\n",
            "WHY DID CHATGPT COULDN'T REPLACE GOOGLE SEARCH?\n",
            "HOW WAS CHATGPT TRAINED?\n",
            "LARGE LANGUAGE MODELS\n",
            "WHAT DO LLMS ESSENTIALLY DO? \n",
            "LLMS AS MACHINE LEARNING TASK?\n",
            "LLMS AS DEEP LEARNING TASK?\n",
            "T RAINING DATA FOR LLMS\n",
            "NEXT WORD GENERATION\n",
            "PHASES OF LLM TRAINING\n",
            "PRE-TRAINING\n",
            "MASSIVE AMOUNT OF TEXT DATA \n",
            "FROM INTERNET - BOOKS, \n",
            "RESEARCH PAPERS, WEBSITES\n",
            "MODEL LEARNS TO PREDICT THE \n",
            "NEXT WORD \n",
            "INSTRUCTION FINE TUNING\n",
            "CURATING Q N A DATASET TO \n",
            "TRAIN THE MODEL TO ANSWER \n",
            "QUESTIONS OR  INSTRUCTIONS  \n",
            "MODEL LEARNS TO BECOME A \n",
            "HELPFUL ASSISTANT \n",
            "REINFORCEMENT LEARNING \n",
            "FROM HUMAN FEEDBACK \n",
            "(RLHF)\n",
            "ALIGN THE OUTPUT CLOSER TO \n",
            "HUMAN LIKE RESPONSES\n",
            "RESPONSES ARE UPDATED \n",
            "CONSIDERING HUMAN \n",
            "FEEDBACK AND PREFERENCE.\n",
            "LIMITATION OF LLMS\n",
            "HALLUCINATION 1.\n",
            "MATHEMATICAL PROBLEM SOLVING 2.\n",
            "CONTEXT WINDOW 3.\n",
            "COST 4.\n",
            "HOW TO MAKE LLMS RESPOND BETTER?\n",
            "ZERO-SHOT\n",
            "GIVE SOME INSTRUCTIONS TO \n",
            "SOLVE A TASK.\n",
            "FEW-SHOT\n",
            "GIVE SOME EXAMPLES OF HOW \n",
            "TO SOLVE A TASK.\n",
            "CHAIN-OF-THOUGHT(COT)\n",
            "FOR COMPLEX TASKS- PROMPT \n",
            "AN LLM TO <THINK STEP BY \n",
            "STEP=\n",
            "LATEST LLMS & FRAMEWORKS\n",
            "LLMS\n",
            "MISTRAL\n",
            "MIXTRAL\n",
            "LLAMA\n",
            "GEMINI\n",
            "DEEPSEEK FRAMEWORKS\n",
            "TOGETHER AI- HTTPS:/ /WWW.TOGETHER.AI/\n",
            "GROQ- HTTPS:/ /GROQ.COM/\n",
            "REPLICATE- HTTPS:/ /REPLICATE.COM/\n",
            "LITELLM - HTTPS:/ /WWW.LITELLM.AI/\n",
            "HUGGING FACE- HTTPS:/ /HUGGINGFACE.CO/\n",
            "GENERATIVE AI PROJECT LIFECYCLE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "import shutil\n",
        "\n",
        "\n",
        "# def convert_to_uppercase(path):\n",
        "#     # Read the uploaded file\n",
        "#     with open(path, 'r') as file:\n",
        "#         content = file.read()\n",
        "#     upper_case_content = content.upper()\n",
        "#     return upper_case_content\n",
        "\n",
        "# def process_file(file_obj):\n",
        "#    return file_obj.name\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=convert_to_uppercase, #convert_to_lowercase,\n",
        "    inputs=gr.File(label=\"Upload Text File\"),\n",
        "    outputs=gr.Textbox(label=\"Converted Text\"),\n",
        "    title=\"Text Case Converter\",\n",
        "    description=\"Upload a text file to convert all uppercase text to lowercase.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "FZJqhvT4lQfz",
        "outputId": "2cfb79b9-dbee-4113-bf85-c1c6a9d4bcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d48d2794d336b36734.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d48d2794d336b36734.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://d48d2794d336b36734.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio app for NER"
      ],
      "metadata": {
        "id": "FMGk6POLgThd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your function"
      ],
      "metadata": {
        "id": "l_Fv-2YSaLKN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7sfeEVNApIR",
        "outputId": "1ece9aa7-56d4-42ec-a58e-bd8a362ff070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple Inc.', 'ORG'), ('Steve Jobs', 'PERSON'), ('Steve Wozniak', 'PERSON'), ('Cupertino', 'GPE'), ('California', 'GPE'), ('April 1, 1976', 'DATE'), ('Apple', 'ORG'), ('iPhone', 'ORG'), ('iPad', 'ORG'), ('MacBook', 'ORG'), ('Steve Jobs', 'PERSON')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "def perform_ner(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Usage example\n",
        "text = \"\"\"Apple Inc. was founded by Steve Jobs and Steve Wozniak in Cupertino, California on April 1, 1976.\n",
        "Apple is known for its innovative products such as the iPhone, iPad, and MacBook. Steve Jobs likes eating apple.\"\"\"\n",
        "\n",
        "entities = perform_ner(text)\n",
        "\n",
        "print(entities)\n",
        "\n",
        "# Print the entities and their labels\n",
        "# for entity, label in entities:\n",
        "#     print(f\"{entity} ({label})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def perform_ner(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "gr.close_all()\n",
        "demo = gr.Interface(fn=perform_ner,\n",
        "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=5)],\n",
        "                    outputs=[gr.HighlightedText(label=\"Entities found\")],\n",
        "                    title=\"NER on Text\",\n",
        "                    description=\"Extraction of entities from text using spacy model\",\n",
        "                    examples=[\"\"\"Apple Inc. was founded by Steve Jobs and Steve Wozniak in Cupertino, California on April 1, 1976.\n",
        "Apple is known for its innovative products such as the iPhone, iPad, and MacBook. Steve Jobs likes eating apple.\"\"\"],\n",
        "                    allow_flagging = 'never')\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "vuyoamQzH960",
        "outputId": "d03a7bf1-e2be-4557-942c-ecbdc6f185cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7862\n",
            "Closing server running on port: 7861\n",
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f7edeae1e4100a47c1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f7edeae1e4100a47c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chat with an LLM"
      ],
      "metadata": {
        "id": "NSJmoETTdsFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain_community langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5VcuROB8gfbu",
        "outputId": "b25dab07-579b-44a4-8bb2-af629b44f0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m174.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "from langchain import PromptTemplate, HuggingFaceHub\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=openai_key"
      ],
      "metadata": {
        "id": "GEFI1YtX-9Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_llm(question, temp):\n",
        "  template = \"\"\"Question: {question}\n",
        "  Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "  repo_id = \"gpt-4o-mini\"\n",
        "\n",
        "  llm = ChatOpenAI(model=repo_id)\n",
        "  llm_chain = prompt | llm\n",
        "\n",
        "  # initialize HF LLM\n",
        "  answer = llm_chain.invoke(question)\n",
        "  return answer.content"
      ],
      "metadata": {
        "id": "-PHDKQHcizby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who am I?\"\n",
        "chat_llm(question,0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "X8Tdscz3jA-1",
        "outputId": "05e3673c-6f29-45f2-ee96-ea7fe0f64651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sure! To help identify you, we can consider a few questions regarding your characteristics, interests, and experiences. Here are some steps we can go through:\\n\\n1. **Basic Information**: Can you share your name or how you like to be referred to?\\n   \\n2. **Interests**: What are some of your hobbies or passions? Do you have any particular fields of interest like art, science, sports, etc.?\\n\\n3. **Location**: Where are you based? This might provide some context about your environment and culture.\\n\\n4. **Profession**: What do you do for work or study? What field are you involved in?\\n\\n5. **Personality Traits**: How would you describe your personality? Are you more introverted or extroverted? What values are important to you?\\n\\nThinking through these questions can help narrow down who you are. Feel free to answer any of these or provide other information that you think is relevant!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Back to Lesson 2, time flies!\n",
        "import gradio as gr\n",
        "demo = gr.Interface(fn=chat_llm,\n",
        "                    inputs=[gr.Textbox(label=\"Prompt\"),\n",
        "                            gr.Slider(label=\"Temperature\",\n",
        "                                      value=0.5,\n",
        "                                      maximum=1,\n",
        "                                      minimum=0.1)],\n",
        "                    outputs=[gr.Textbox(label=\"Completion\")])\n",
        "\n",
        "# gr.close_all()\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "Xo1T465mlfBL",
        "outputId": "4df39449-7992-4173-b2f1-3419df5e71ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c7885f30076b6b2503.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c7885f30076b6b2503.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradio chatbot"
      ],
      "metadata": {
        "id": "aLfedrHG9kJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def respond(message, chat_history):\n",
        "        #No LLM here, just respond with a random pre-made message\n",
        "        bot_message = random.choice([\"Hi, Good evening!\",\n",
        "                                     \"Hey, how are you?\",\n",
        "                                     \"Hello, hope you are doing well!\"])\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
        "    msg = gr.Textbox(label=\"Prompt\")\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
        "\n",
        "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
        "\n",
        "gr.close_all()\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "E3TN25CJ9lzD",
        "outputId": "277cd8d2-ed32-4dda-a1da-663f24273320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-94a2318e1534>:12: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=240) #just to fit the notebook\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7862\n",
            "Closing server running on port: 7861\n",
            "Closing server running on port: 7862\n",
            "Closing server running on port: 7861\n",
            "Closing server running on port: 7860\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://41ac68d8c39cf3bcf3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://41ac68d8c39cf3bcf3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wFremJdZ-I8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}